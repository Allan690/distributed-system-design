{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Web Crawler\n",
    "\n",
    "Let's design a Web Crawler that will browse and download the World Wide Web. \n",
    "\n",
    "## What's a Web Crawler?\n",
    "It's a software program which browses the WWW in a methodical and automated manner, collecting documents by recursively fetching links from a set of starting pages.\n",
    "\n",
    "Search engines use web crawling as a means to provide up-to-date data. Search engines download all pages and create an index on them to perform faster searches.\n",
    "\n",
    "Other uses of web crawlers:\n",
    "- Test web pages and links for valid syntax and structure.\n",
    "- To search for copyright infringements.\n",
    "- To maintain mirror sites for popular web sites.\n",
    "- To monitor sites to see where their content changes.\n",
    "\n",
    "## 1. Requirements and Goals of the System\n",
    "**Scalability:** Our service needs to be scalable, since we'll be fetching hundreds of millions of web documents.\n",
    "\n",
    "**Extensibility:** Our service should be designed in a way that allows newer functionality to be added to it. It should be able to allow for newer document formats that needs to be downloaded and processed in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design Considerations\n",
    "We should be asking a few questions here:\n",
    "\n",
    "#### Is it a crawler for only HTML pages? Or should we fetch and store other media types like images, videos, etc.\n",
    "It's important to clarify this because it will change the design. If we're writing a general-purpose crawler, we might want to break down the parsing module into different sets of modules: one for HTML, another for videos,..., so basically each module handling a given media type.\n",
    "\n",
    "For this design, let's assume our web crawler will deal with HTML only.\n",
    "\n",
    "#### What protocols apart from HTTP are we looking at? FTP?\n",
    "Let's assume HTTP for now. Again, it should not be hard to extend it to other protocols.\n",
    "\n",
    "#### What is the expected number of pages we will crawl? How big will be the URL Database?\n",
    "Let's assume we need to crawl 1Billion websites. Since one site can contain many URLs, assume an upper bound of `15 billion web pages`.\n",
    "\n",
    "\n",
    "#### Robots Exclusion Protocol?\n",
    "Some web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web Crawler to fetch a document called `robot.txt` which contains these declarations for that site before downloading any real content from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Capacity Estimation and Constraints\n",
    "If we crawl 15B pages in 4 weeks, how many pages will we need to fetch per second?\n",
    "\n",
    "```text\n",
    "        15B / (4 weeks * 7 days * 86400 sec) ~= 6200 web pages/sec\n",
    "```\n",
    "\n",
    "**What about storage?** Pages sizes vary. But since we are dealing with HTML only, let's assume an average page size is 100KB. With each page, if we're storing 500 bytes of metadata, total storage we would need is:\n",
    "\n",
    "```text\n",
    "        15B * (100KB + 500 bytes)\n",
    "        15 B * 100.5 KB ~= 1.5 Petabytes\n",
    "```\n",
    "\n",
    "We don't want to go beyond 70% capacity of our storage system, so the total storage we will need is:\n",
    "\n",
    "```text\n",
    "        1.5 petabytes / 0.7 ==> 2.14 Petabytes     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. High Level Design\n",
    "The basic algorithm of a web crawler is this:\n",
    "\n",
    "1. Taking in a list of seed URLs as input, pick a URL from the unvisited URL list.\n",
    "2. Find the URL host-name's IP address.\n",
    "3. Establish a connection to the host to download its corresponding documents.\n",
    "4. Parse the documents contents to look for new URLs.\n",
    "5. Add the new URLs to the list of unvisited URLs.\n",
    "6. Process the downloaded document, e.g, store it, or index the contents\n",
    "7. Go back to step 1.\n",
    "\n",
    "### How to Crawl\n",
    "\n",
    "Breath first or depth first?\n",
    "Breadth-first search (BFS) is usually used. We can also use Depth-first search especially when the crawler has already established a connection with a website. In this situation, the crawler will just DFS all the URLs within the website to save some handshaking overhead.\n",
    "\n",
    "**Path-ascending crawling:** Path-ascending crawling helps discover a hidden or isolated resources. In this scheme, a crawler would ascend to every path in each URL like so:\n",
    "```text\n",
    "    given a seed URL of http://xyz.com/a/b/one.html\n",
    "\n",
    "    it will attempt to crawl /a/b/, /a/ and /\n",
    "```\n",
    "\n",
    "### Difficulties implementing an efficient web crawler.\n",
    "#### 1. Large volume of web pages\n",
    "A large volume implies that the web crawler can only dowload a fraction of the web pages, so it's critical that the web crawler should be intelligent enough to prioritize download.\n",
    "\n",
    "#### 2. Rate of change on web pages\n",
    "Web pages change frequenty. By the time the crawler is downloading the last page from the site, the page may change dynamically, or a new page may be added.\n",
    "\n",
    "**Components of a bare minimum crawler:**\n",
    "1. **URL frontier:** stores a list of URLs to download and prioritize which URLs should be crawled first.\n",
    "2. **HTTP Fetcher:** to retrieve a web page from the hosts server.\n",
    "3. **Extractor:** to extract links from HTML documents.\n",
    "4. **Duplicate Remover:** to make sure same content is not extracted twice.\n",
    "5. **Datastore:** to store retrieved pages, URLs and other metadata.\n",
    "\n",
    "![](images/designing_webcrawler_high_level.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
