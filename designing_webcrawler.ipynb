{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Web Crawler\n",
    "\n",
    "Let's design a Web Crawler that will browse and download the World Wide Web. \n",
    "\n",
    "## What's a Web Crawler?\n",
    "It's a software program which browses the WWW in a methodical and automated manner, collecting documents by recursively fetching links from a set of starting pages.\n",
    "\n",
    "Search engines use web crawling as a means to provide up-to-date data. Search engines download all pages and create an index on them to perform faster searches.\n",
    "\n",
    "Other uses of web crawlers:\n",
    "- Test web pages and links for valid syntax and structure.\n",
    "- To search for copyright infringements.\n",
    "- To maintain mirror sites for popular web sites.\n",
    "- To monitor sites to see where their content changes.\n",
    "\n",
    "## 1. Requirements and Goals of the System\n",
    "**Scalability:** Our service needs to be scalable, since we'll be fetching hundreds of millions of web documents.\n",
    "\n",
    "**Extensibility:** Our service should be designed in a way that allows newer functionality to be added to it. It should be able to allow for newer document formats that needs to be downloaded and processed in future.\n",
    "\n",
    "## 2. Design Considerations\n",
    "We should be asking a few questions here:\n",
    "\n",
    "#### Is it a crawler for only HTML pages? Or should we fetch and store other media types like images, videos, etc.\n",
    "It's important to clarify this because it will change the design. If we're writing a general-purpose crawler, we might want to break down the parsing module into different sets of modules: one for HTML, another for videos,..., so basically each module handling a given media type.\n",
    "\n",
    "For this design, let's assume our web crawler will deal with HTML only.\n",
    "\n",
    "#### What protocols apart from HTTP are we looking at? FTP?\n",
    "Let's assume HTTP for now. Again, it should not be hard to extend it to other protocols.\n",
    "\n",
    "#### What is the expected number of pages we will crawl? How big will be the URL Database?\n",
    "Let's assume we need to crawl 1Billion websites. Since one site can contain many URLs, assume an upper bound of `15 billion web pages`.\n",
    "\n",
    "\n",
    "#### Robots Exclusion Protocol?\n",
    "Some web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web Crawler to fetch a document called `robot.txt` which contains these declarations for that site before downloading any real content from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
