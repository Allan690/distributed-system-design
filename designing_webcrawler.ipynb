{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Web Crawler\n",
    "\n",
    "Let's design a Web Crawler that will browse and download the World Wide Web. \n",
    "\n",
    "## What's a Web Crawler?\n",
    "It's a software program which browses the WWW in a methodical and automated manner, collecting documents by recursively fetching links from a set of starting pages.\n",
    "\n",
    "Search engines use web crawling as a means to provide up-to-date data. Search engines download all pages and create an index on them to perform faster searches.\n",
    "\n",
    "Other uses of web crawlers:\n",
    "- Test web pages and links for valid syntax and structure.\n",
    "- To search for copyright infringements.\n",
    "- To maintain mirror sites for popular web sites.\n",
    "- To monitor sites to see where their content changes.\n",
    "\n",
    "## 1. Requirements and Goals of the System\n",
    "**Scalability:** Our service needs to be scalable, since we'll be fetching hundreds of millions of web documents.\n",
    "\n",
    "**Extensibility:** Our service should be designed in a way that allows newer functionality to be added to it. It should be able to allow for newer document formats that needs to be downloaded and processed in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design Considerations\n",
    "We should be asking a few questions here:\n",
    "\n",
    "#### Is it a crawler for only HTML pages? Or should we fetch and store other media types like images, videos, etc.\n",
    "It's important to clarify this because it will change the design. If we're writing a general-purpose crawler, we might want to break down the parsing module into different sets of modules: one for HTML, another for videos,..., so basically each module handling a given media type.\n",
    "\n",
    "For this design, let's assume our web crawler will deal with HTML only.\n",
    "\n",
    "#### What protocols apart from HTTP are we looking at? FTP?\n",
    "Let's assume HTTP for now. Again, it should not be hard to extend it to other protocols.\n",
    "\n",
    "#### What is the expected number of pages we will crawl? How big will be the URL Database?\n",
    "Let's assume we need to crawl 1Billion websites. Since one site can contain many URLs, assume an upper bound of `15 billion web pages`.\n",
    "\n",
    "\n",
    "#### Robots Exclusion Protocol?\n",
    "Some web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web Crawler to fetch a document called `robot.txt` which contains these declarations for that site before downloading any real content from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Capacity Estimation and Constraints\n",
    "If we crawl 15B pages in 4 weeks, how many pages will we need to fetch per second?\n",
    "\n",
    "```text\n",
    "        15B / (4 weeks * 7 days * 86400 sec) ~= 6200 web pages/sec\n",
    "```\n",
    "\n",
    "**What about storage?** Pages sizes vary. But since we are dealing with HTML only, let's assume an average page size is 100KB. With each page, if we're storing 500 bytes of metadata, total storage we would need is:\n",
    "\n",
    "```text\n",
    "        15B * (100KB + 500 bytes)\n",
    "        15 B * 100.5 KB ~= 1.5 Petabytes\n",
    "```\n",
    "\n",
    "We don't want to go beyond 70% capacity of our storage system, so the total storage we will need is:\n",
    "\n",
    "```text\n",
    "        1.5 petabytes / 0.7 ==> 2.14 Petabytes     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. High Level Design\n",
    "The basic algorithm of a web crawler is this:\n",
    "\n",
    "1. Taking in a list of seed URLs as input, pick a URL from the unvisited URL list.\n",
    "2. Find the URL host-name's IP address.\n",
    "3. Establish a connection to the host to download its corresponding documents.\n",
    "4. Parse the documents contents to look for new URLs.\n",
    "5. Add the new URLs to the list of unvisited URLs.\n",
    "6. Process the downloaded document, e.g, store it, or index the contents\n",
    "7. Go back to step 1.\n",
    "\n",
    "### How to Crawl\n",
    "\n",
    "Breath first or depth first?\n",
    "Breadth-first search (BFS) is usually used. We can also use Depth-first search especially when the crawler has already established a connection with a website. In this situation, the crawler will just DFS all the URLs within the website to save some handshaking overhead.\n",
    "\n",
    "**Path-ascending crawling:** Path-ascending crawling helps discover a hidden or isolated resources. In this scheme, a crawler would ascend to every path in each URL like so:\n",
    "```text\n",
    "    given a seed URL of http://xyz.com/a/b/one.html\n",
    "\n",
    "    it will attempt to crawl /a/b/, /a/ and /\n",
    "```\n",
    "\n",
    "### Difficulties implementing an efficient web crawler.\n",
    "#### 1. Large volume of web pages\n",
    "A large volume implies that the web crawler can only dowload a fraction of the web pages, so it's critical that the web crawler should be intelligent enough to prioritize download.\n",
    "\n",
    "#### 2. Rate of change on web pages\n",
    "Web pages change frequenty. By the time the crawler is downloading the last page from the site, the page may change dynamically, or a new page may be added.\n",
    "\n",
    "**Components of a bare minimum crawler:**\n",
    "1. **URL frontier:** stores a list of URLs to download and prioritize which URLs should be crawled first.\n",
    "2. **HTTP Fetcher:** to retrieve a web page from the hosts server.\n",
    "3. **Extractor:** to extract links from HTML documents.\n",
    "4. **Duplicate Remover:** to make sure same content is not extracted twice.\n",
    "5. **Datastore:** to store retrieved pages, URLs and other metadata.\n",
    "\n",
    "![](images/designing_webcrawler_high_level.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Component Design\n",
    "Assume the crawler is running on a single server, where multiple working threads are performing all the steps needed to download and process a document in a loop.\n",
    "\n",
    "**Step 1:** remove an absolute URL from the shared URL frontier for downloading. the URL begins with a scheme (e.g HTTP) which identifies the network protocol that should be used to download it.\n",
    "We can implement these protocols in a modular way for extensibility, so that later if our crawler needs to support more protocols, it can easily be done.\n",
    "\n",
    "**Step 2:** Based on the URL's scheme, the worker calls the appropriate protocol module to download the document.\n",
    "\n",
    "**Step 3:** After downloading, the document is written into a Document Input Stream (DIS). This will enable other modules to re-read the document multiple times.\n",
    "\n",
    "**Step 4:** The worker invokes the dedupe test to see whether this document (associated with a different URL) has already been seen before. If so, the document is not processed any further and the worker thread removes the next URL from the frontier.\n",
    "\n",
    "**Step 5:** Process the downloaded document. Each doc has a different MIME type like HTML page, Image, Video etc. We can implement these MIME schemes in a modular way, to allow for extensibility when our crawler need to support more types. The worker invokes the process method of each processing module with that MIME type.\n",
    "\n",
    "**Step 6:** The HTML processing module will extract links from the page. Each link is converted into an absolute URL and testsed against a user-supplied filter to determine if it should be downloaded. If the URL passes the filter, the worker performs the URL-dedupe test, which checks if the URL has been downloaded before. If it's new, it is added into the URL frontier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
