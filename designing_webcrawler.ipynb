{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Web Crawler\n",
    "\n",
    "Let's design a Web Crawler that will browse and download the World Wide Web. \n",
    "\n",
    "## What's a Web Crawler?\n",
    "It's a software program which browses the WWW in a methodical and automated manner, collecting documents by recursively fetching links from a set of starting pages.\n",
    "\n",
    "Search engines use web crawling as a means to provide up-to-date data. Search engines download all pages and create an index on them to perform faster searches.\n",
    "\n",
    "Other uses of web crawlers:\n",
    "- Test web pages and links for valid syntax and structure.\n",
    "- To search for copyright infringements.\n",
    "- To maintain mirror sites for popular web sites.\n",
    "- To monitor sites to see where their content changes.\n",
    "\n",
    "## 1. Requirements and Goals of the System\n",
    "**Scalability:** Our service needs to be scalable, since we'll be fetching hundreds of millions of web documents.\n",
    "\n",
    "**Extensibility:** Our service should be designed in a way that allows newer functionality to be added to it. It should be able to allow for newer document formats that needs to be downloaded and processed in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design Considerations\n",
    "We should be asking a few questions here:\n",
    "\n",
    "#### Is it a crawler for only HTML pages? Or should we fetch and store other media types like images, videos, etc.\n",
    "It's important to clarify this because it will change the design. If we're writing a general-purpose crawler, we might want to break down the parsing module into different sets of modules: one for HTML, another for videos,..., so basically each module handling a given media type.\n",
    "\n",
    "For this design, let's assume our web crawler will deal with HTML only.\n",
    "\n",
    "#### What protocols apart from HTTP are we looking at? FTP?\n",
    "Let's assume HTTP for now. Again, it should not be hard to extend it to other protocols.\n",
    "\n",
    "#### What is the expected number of pages we will crawl? How big will be the URL Database?\n",
    "Let's assume we need to crawl 1Billion websites. Since one site can contain many URLs, assume an upper bound of `15 billion web pages`.\n",
    "\n",
    "\n",
    "#### Robots Exclusion Protocol?\n",
    "Some web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web Crawler to fetch a document called `robot.txt` which contains these declarations for that site before downloading any real content from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Capacity Estimation and Constraints\n",
    "If we crawl 15B pages in 4 weeks, how many pages will we need to fetch per second?\n",
    "\n",
    "```text\n",
    "        15B / (4 weeks * 7 days * 86400 sec) ~= 6200 web pages/sec\n",
    "```\n",
    "\n",
    "**What about storage?** Pages sizes vary. But since we are dealing with HTML only, let's assume an average page size is 100KB. With each page, if we're storing 500 bytes of metadata, total storage we would need is:\n",
    "\n",
    "```text\n",
    "        15B * (100KB + 500 bytes)\n",
    "        15 B * 100.5 KB ~= 1.5 Petabytes\n",
    "```\n",
    "\n",
    "We don't want to go beyond 70% capacity of our storage system, so the total storage we will need is:\n",
    "\n",
    "```text\n",
    "        1.5 petabytes / 0.7 ==> 2.14 Petabytes     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. High Level Design\n",
    "The basic algorithm of a web crawler is this:\n",
    "\n",
    "1. Taking in a list of seed URLs as input, pick a URL from the unvisited URL list.\n",
    "2. Find the URL host-name's IP address.\n",
    "3. Establish a connection to the host to download its corresponding documents.\n",
    "4. Parse the documents contents to look for new URLs.\n",
    "5. Add the new URLs to the list of unvisited URLs.\n",
    "6. Process the downloaded document, e.g, store it, or index the contents\n",
    "7. Go back to step 1.\n",
    "\n",
    "### How to Crawl\n",
    "\n",
    "Breath first or depth first?\n",
    "Breadth-first search (BFS) is usually used. We can also use Depth-first search especially when the crawler has already established a connection with a website. In this situation, the crawler will just DFS all the URLs within the website to save some handshaking overhead.\n",
    "\n",
    "**Path-ascending crawling:** Path-ascending crawling helps discover a hidden or isolated resources. In this scheme, a crawler would ascend to every path in each URL like so:\n",
    "```text\n",
    "    given a seed URL of http://xyz.com/a/b/one.html\n",
    "\n",
    "    it will attempt to crawl /a/b/, /a/ and /\n",
    "```\n",
    "\n",
    "### Difficulties implementing an efficient web crawler.\n",
    "#### 1. Large volume of web pages\n",
    "A large volume implies that the web crawler can only dowload a fraction of the web pages, so it's critical that the web crawler should be intelligent enough to prioritize download.\n",
    "\n",
    "#### 2. Rate of change on web pages\n",
    "Web pages change frequenty. By the time the crawler is downloading the last page from the site, the page may change dynamically, or a new page may be added.\n",
    "\n",
    "**Components of a bare minimum crawler:**\n",
    "1. **URL frontier:** stores a list of URLs to download and prioritize which URLs should be crawled first.\n",
    "2. **HTTP Fetcher:** to retrieve a web page from the hosts server.\n",
    "3. **Extractor:** to extract links from HTML documents.\n",
    "4. **Duplicate Remover:** to make sure same content is not extracted twice.\n",
    "5. **Datastore:** to store retrieved pages, URLs and other metadata.\n",
    "\n",
    "![](images/designing_webcrawler_high_level.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Component Design\n",
    "Assume the crawler is running on a single server, where multiple working threads are performing all the steps needed to download and process a document in a loop.\n",
    "\n",
    "**Step 1:** remove an absolute URL from the shared URL frontier for downloading. the URL begins with a scheme (e.g HTTP) which identifies the network protocol that should be used to download it.\n",
    "We can implement these protocols in a modular way for extensibility, so that later if our crawler needs to support more protocols, it can easily be done.\n",
    "\n",
    "**Step 2:** Based on the URL's scheme, the worker calls the appropriate protocol module to download the document.\n",
    "\n",
    "**Step 3:** After downloading, the document is written into a Document Input Stream (DIS). This will enable other modules to re-read the document multiple times.\n",
    "\n",
    "**Step 4:** The worker invokes the dedupe test to see whether this document (associated with a different URL) has already been seen before. If so, the document is not processed any further and the worker thread removes the next URL from the frontier.\n",
    "\n",
    "**Step 5:** Process the downloaded document. Each doc has a different MIME type like HTML page, Image, Video etc. We can implement these MIME schemes in a modular way, to allow for extensibility when our crawler need to support more types. The worker invokes the process method of each processing module with that MIME type.\n",
    "\n",
    "**Step 6:** The HTML processing module will extract links from the page. Each link is converted into an absolute URL and testsed against a user-supplied filter to determine if it should be downloaded. If the URL passes the filter, the worker performs the URL-dedupe test, which checks if the URL has been downloaded before. If it's new, it is added into the URL frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/designing_crawler_detailed_component.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how each component can be distributed onto multiple machines:\n",
    "\n",
    "#### 1. The URL frontier\n",
    "This is the data structure that contains all the URLs that are queued to be downloaded. We crawl by performing a breadth-first traversal of the Web, starting from the pages in the seed set. We can use a FIFO queue to implement this.\n",
    "\n",
    "Since we have a huge list of URLs to crawl, we can distribute our URL frontier into multiple servers. Let's assume on each server we have multiple threads performing the crawling tasks. Our hash function maps each URL to the server responsible for crawling it.\n",
    "\n",
    "Constraints for a distributed URL frontier:\n",
    "- The crawler should not overload a server by downloading a lot of pages.\n",
    "- Multiple machines should not connect to a single web server.\n",
    "\n",
    "> For each server, we can have distinct FIFO sub-queues, where each worker thread will remove URLs for crawling.\n",
    "\n",
    "Once a new URL is added, we determine which sub-queue it belongs to by using the URL's canonical hostname. Our hash function will map each **hostname** to a **thread number**. Together, these two points imply, that only one worker thread will download documents from a given web server and also, by using FIFO queue, it'll not overload a web server.\n",
    "\n",
    "##### How big will our URL frontier be?\n",
    "The size would be in the 100s of millions of URLs. Therefore, we need to store the URLs on disk. We can implement our queues in such a way that they have separate buffers for enqueuing and dequeuing. Enqueuing buffer, once filled, will be dumped to the disk. Dequeuing buffers will keep a cache of URLs that need to be visited; periodically reading from the disk to fill the buffer.\n",
    "\n",
    "#### 2. The Fetcher Module\n",
    "This will download the document corresponding to a given URL using the appropriate network protocol like HTTP. Webmasters create a `robot.txt` to make certain parts of the websites off limits for the crawler. \n",
    "To avoid downloading this text file on every request, our HTTP protocol module can maintain a cache mapping host-names to their robot's exclusion rules. \n",
    "\n",
    "#### 3. Document input steam\n",
    "We cache the document locally using DIS to avoid downloading the document multiple times.\n",
    "\n",
    "A DIS is an input stream that caches the doc's entire contents in memory. It also provides methods to re-read the document. Larger documents can be temporarily written to a backing file.\n",
    "\n",
    "Each worker will have a DIS, which it reuses from document to document. After extracting a URL from the frontier, the worker passes that URL to the relevant protocol module (in our case, for HTTP) which initializes the DIS from a network connection to contain the document's contents. The worker then passes the DIS to all relevant processing modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
